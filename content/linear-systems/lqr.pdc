---
title: Linear Quadratic Regulation (LQR)
weight: 01
categories:
  - MDP
tags:
  - Linear systems
  - Riccati equation
  - LQR
  - Optimal tracking
---

_Note: To be consistent with the notation used in linear systems, we denote
the state and action by lowercase $x$ and $u$, even for stochastic systems
(unlike the notation used for other models where we use uppercase $X$ and $U$
for state and actions to emphasize the fact they are random variables)._

We start by considering a _determinisitc_ linear system with state $x_t \in
\reals^n$ and control actions $u_t \in \reals^m$ and dynamics
$$ x_{t+1} = A_t x_t + B_t u_t,$$
where $A_t \in \reals^{n \times n}$ and $B_t \in \reals^{n \times m}$ are
known matrices. The objective is to choose the control actions to minimize the
finite horizon cost given by 
$$ \sum_{t=1}^{T-1} c_t(x_t, u_t) + c_T(x_T),$$
where $c_t(x_t, u_t)$ is the per-step cost and $c_T(x_T)$ is the terminal
cost. Depending on the cost, such models can be classified as follows:

* **Regulation problem** where the objective is to keep the state of the
  system close to origin. These are modeled by considering that there are
  sequence of [positive semi-definite matrices][PD] $\{Q_t\}_{t=1}^{T}$ and 
  [positive definite matrices][PD] $\{R_t\}_{t=1}^T$ where
  $$
    c_t(x_t, u_t) = x_t^\TRANS Q_t x_t + u_t^\TRANS R_t u_t
    \quad\text{and}\quad
    c_T(x_T) = x_T^\TRANS Q_T x_T.
  $$
  This is often referred to as the _Linear Quadratic Regulartor (LQR)_.

* **Tracking problem** where it is assumed that the system has an output $y_t
  = C_t x_t$ and the objective is to keep the output of the system
  close to a pre-specified trajectory $\{y^\circ_t\}_{t=1}^T$. These are
  modeled by considering the per-step cost as
  $$ \begin{align*}
    c_t(x_t, u_t) &= (C_tx_t - y^\circ_t)^\TRANS Q_t (C_tx_t - y^\circ_t) + u_t^\TRANS R_t u_t
    \\
    \text{and}\quad
    c_T(x_T) &= (C_Tx_T - y^\circ_T)^\TRANS Q_T (C_Tx_T - y^\circ_T).
  \end{align*} $$

[PD]: ../../appendix/positive-definite-matrix

## Optimal solution of the regulation problem

The LQR problem is one of the few instances where the solution of the Markov
decision process can be obtained in closed form. 

::: highlight :::
Theorem #thm:lqr

:   The value function at time $t$ is
    $$\begin{equation}\label{eq:Vt}
      V_t(x_t) = x_t^\TRANS S_t x_t
    \end{equation}$$
    and the optimal control action is
    $$\begin{equation}\label{eq:gt}
      u_t = - K_t x_t
    \end{equation}$$
    where the _gain matrices_ $\{K_t\}_{t\ge 1}$ are given by:
    $$ K_t = [R_t + B_t^\TRANS S_{t+1} B_t]^{-1} \Lambda_t $$
    where
    $$ \Lambda_t = B_t^\TRANS S_{t+1} A_t $$
    and $\{S_t\}_{t \ge 1}$ are determined by the _backward Riccati difference
    equations_: $S_T = Q_T$ and for $t \in \{T-1, \dots, 1\}$: 
    $$\begin{equation}\label{eq:riccati}
      S_t = A_t^\TRANS S_{t+1} A_t + Q_t - 
      \Lambda_t^\TRANS [ R_t + B_t^\TRANS S_{t+1} B_t ] ^{-1} \Lambda_t. 
    \end{equation}$$
:::


Side remark

:   **Riccati equations** are named after _Jacopo Riccati_
    (1670--1754) who studied the differential equations of the form 
    $$\dot x = a x^2 + b t + c t^2$$
    and its variations. In modern control, such equations arise in the
    calculus of variations and optimal filtering. The discrete-time version of
    these equations are also named after Riccati.

Since the LQR model is really simple, there are multiple ways in which the
above result can be proved. We first present a dynamic programming based
proof. The proof relies on the following completion of squares lemma.

::: highlight :::
Lemma

:   **(Completion of squares)**

    For any $x \in \reals^n$ and $u \in \reals^m$ and matrices $A$, $B$, $S$, and
    $R$ of appropriate dimensions, we have
    $$\begin{align*}
      &u^\TRANS R u + (Ax + Bu)^\TRANS S (Ax + Bu) 
      \\ &\qquad = 
     (u + K x)^\TRANS [R + B^\TRANS S B] (u + Kx) + x^\TRANS L x
     \end{align*}$$
    where 

    *   $L = A^\TRANS S A - \Lambda^\TRANS[R + B^\TRANS S B]^{-1} \Lambda_t$
    *   $K = [R + B^\TRANS S B]^{-1} \Lambda$
    *   $\Lambda = B^\TRANS S A$

:::

#### Proof

The proof follows immediately by completing the square on the left hand side.
In particular

$$ \begin{align*}
& u^\TRANS R u +  (Ax+Bu)^\TRANS S (Ax + Bu) \\
& \quad = u^\TRANS (R + B^\TRANS S B) u + 2 u^\TRANS B^\TRANS S A x 
  + x^\TRANS A^\TRANS S A x \\
& \quad = u^\TRANS [R + B^\TRANS S B] u + 2 u^\TRANS B^\TRANS S A x 
  + x^\TRANS K^\TRANS [R + B^\TRANS S B] K x + x^\TRANS L x \\
& \quad = 
  (u + K x)^\TRANS [R + B^\TRANS S B] (u + Kx) + x^\TRANS L x.
\end{align*} $$

#### Proof of dynamic programming decomposition

The proof of the dynamic program now follows from backward induction. 

For $t=T$, we have
$$ V_T(x) = c_T(x) = x^\TRANS Q_T x $$
which forms the basis of induction. Assume that \\eqref{eq:Vt} holds for
$t+1$ and consider the value function at time $t$. 
$$\begin{align*}
  V_{t}(x) &= \min_{u \in \reals^m} \Big\{
  x^\TRANS Q_t x + u^\TRANS R_t u + V_{t+1}(Ax + Bu) \Big\} \\
  &\stackrel{(a)}= \min_{u \in \reals^m} \Big\{
  x^\TRANS Q_t x + u^\TRANS R_t u + (Ax + Bu)^\TRANS S_{t+1} (Ax + Bu) \Big\} \\
  &\stackrel{(b)}= \min_{u \in \reals^m} \Big\{
  x^\TRANS Q_t x + (u + K_t x)^\TRANS [B_t^\TRANS S_{t+1} B_t + R_t] (u+ K_tx)
  + x^\TRANS L_t x \Big\}
  \\
  &\stackrel{(c)}=x^\TRANS (Q_t + L_t) x
\end{align*}$$
where $(a)$ follows from the induction hypothesis, $(b)$ follows from
completion of squares lemma with $K_t$ and $L_t$ defined similar to the lemma,
and $(c)$ follows from minimizing over $u$, where the minima is achieved by
choosing $u = -K_t x$. This proves the induction step.

## Linear quadratic tracking {#LQT}

Now consider the (simplified form of) LQR tracking problem where we want to ensure that the state
signal $\{x_t\}_{t \ge 1}$ is close to a reference trajectory $\{x^\circ_t\}_{t
\ge 1}$. The per step cost function is 
$$ c_t(x_t, u_t) = (x_t - x^\circ_t)^\TRANS Q_t (x_t - x^\circ_t) +
u_t^\TRANS R_t u_t$$
and the terminal cost is
$$ c_T(x_T) = (x_T - x^\circ_T)^\TRANS Q_T (x_T - x^\circ_T).$$

::: highlight :::
Theorem #thm:lqt

:   The value function at time $t$ is
    $$ V_t(x) = x^\TRANS S_t x - 2 x^\TRANS r_t + ρ_t $$
    and the optimal control action is 
    $$ u_t = - K_t x_t + K^\circ_t r_{t+1}$$
    where
    $\{S_t\}_{t=1}^T$ and $\{K_t\}_{t=1}^T$ follow the same recursions as
    before. The gain matrices $\{K^\circ_t\}$ are given by
    $$ K^\circ_t = [R_t + B_t^\TRANS S_{t+1} B_t]^{-1} B_t^\TRANS $$
    and the correction terms $\{r_t\}$ are given by 
    $$ \begin{align*}
      r_T &= Q_T x^\circ_T \\
      r_t &= [A_t - B_t K_t]^\TRANS r_{t+1} + Q_t x^\circ_t
    \end{align*} $$
    and the tracking error $\{ρ_t\}_{t=1}^T$ is given by
    $$ \begin{align*}
      ρ_T &= (x^\circ_T)^\TRANS Q_T x^\circ_T, \\
      ρ_t &= (x^\circ_t)^\TRANS Q_t x^\circ_t  
      - 2 r^\TRANS_{t+1} B_t [R_t + B_t^\TRANS S_{t+1} B_t]^{-1} B_t^\TRANS
        r_{t+1}
      + ρ_{t+1}.
    \end{align*} $$

:::

The proof follows from backward induction and basic algebra and is left as an
exercise.

## Stochastic linear quadratic regulator

Now consider a system with stochastic dynamics
$$ x_{t+1} = A_t x_t + B_t u_t + w_t $$
where $\{w_1,\dots,w_T\}$ are independent random variables with zero mean
and finite variance
given by $\EXP[w_t^\TRANS w_t] = \Sigma^w_t$. 

::: highlight :::
Theorem #thm:stochastic-lqr

:   The value function at time $t$ is
    $$\begin{equation}\label{eq:Vt-s}
      V_t(x_t) = x_t^\TRANS S_t x_t + α_t
    \end{equation}$$
    and the optimal control action is
    $$\begin{equation}\label{eq:gt-s}
      u_t = - K_t x_t
    \end{equation}$$
    where the matrices $\{K_t\}_{t\ge 1}$ and $\{S_t\}_{t \ge 1}$ follow 
    _the same recursion_ as before and the sequence $\{α_t\}_{t=1}^T$ is
    computed in a backward manner as follows: $α_T = 0$ and 
    $$α_t = α_{t+1} + \TR(Σ^w_t S_{t+1}) = \sum_{τ=t+1}^{T-1} \TR(Σ^w_t S_{t+1}).$$
:::

The proof is similar to the deterministic case and relies on the following
observation.

Lemma

:   For any deterministic value of $x \in \reals^n$ and a random zero
    mean variable $w \in \reals^n$ with finite variance given by
    $\EXP[w^\TRANS w] = Σ^w$, we have 
    $$ \EXP[ (x + w)^\TRANS Q (x+w) ] = x^\TRANS Q x + \TR(Q Σ^w). $$


#### Proof

Note that

$$\begin{align*}
  \EXP[ (x+w)^\TRANS Q (x+w) ] &= 
  \EXP[ x^\TRANS Q x + 2 x^\TRANS Q w + w^\TRANS Q w ] \\
  &= x^\TRANS Q x + \TR(Σ^w Q).
\end{align*}$$
where the second term is zero because $w$ is a zero mean random variable. For
the third term, we are using
$$ \begin{align*}
  \EXP[ w^\TRANS Q w]  &= 
  \EXP\bigg[ \sum_{i=1}^n \sum_{j=1}^n w_i Q_{ij} w_j \bigg]  \\
  &= \sum_{i=1}^n \sum_{j=1}^n \EXP[w_i w_j] Q_{ij} \\
  &= \sum_{i=1}^n \sum_{j=1}^n Σ^w_{ij} Q_{ij} \\
  &= \TR(Σ^w Q)
\end{align*} $$

#### Proof of stochastic LQR

Using the above lemma, we can prove the result for stochastic LQR using
backward induction. The details are left as an exercise.

---

## An alternative proof for stochastic LQR without using dynamic programming {#astrom}

We now present a proof of the stochastic LQR that does not use dynamic
programming. This is based on a simple idea of transforming the total cost
using the solution of the Riccati equation.

::: highlight :::
Prop. #prop:astrom

:   For any control strategy, the total cost 
    $$ \EXP\bigg[\sum_{t=1}^{T-1} \big[ x_t^\TRANS Q_t x_t + u_t^\TRANS R_t
    u_t \big] + x_T^\TRANS Q_T x_T \bigg]$$
    may be written as
    $$ \begin{align}
        & \EXP\bigg[ \sum_{t=1}^{T-1} (u_t + K_t x_t)^\TRANS [R_t + B_t^\TRANS
        S_{t+1}B_t] (u_t + K_t x_t) \bigg] \nonumber \\
        & \quad + 
        \EXP\bigg[ x_1^\TRANS S_1 x_1 + \sum_{t=1}^{T-1} w_t^\TRANS S_{t+1} w_t \bigg], \label{eq:astrom}
    \end{align}$$
    where the matrices $\{K_t\}_{t\ge 1}$ and $\{S_t\}_{t \ge 1}$ follow are
    given as in @thm:lqr.
:::

### Proof

This result can be obtained by repeatedly applying the completion of squares
lemma. In particular, note that $S_T = Q_T$ and using the telescopic sum, we
can write
$$\begin{equation}\label{eq:astrom-1}
  x_T^\TRANS Q_T x_T = x_1^\TRANS S_1 x_1 + 
  \sum_{t=1}^{T-1} \big[ x_{t+1}^\TRANS S_{t+1} x_{t+1} - x_t^\TRANS S_t x_t \big].
\end{equation}$$

Furthermore, since $w_t$ is independet of $x_t$, we have
$$ \begin{align}
  \EXP[ x_{t+1}^\TRANS S_{t+1} x_{t+1}] &= 
   \EXP[ (A_t x_t + B_t u_t + w_t)^\TRANS S_{t+1} (A_t x_t + B_t u_t + w_t) 
   \nonumber \\
   &= \EXP[ (A_t x_t + B_t u_t)^\TRANS S_{t+1} (A_t x_t + B_t u_t) ]
     + \EXP[ w_t^\TRANS S_{t+1} w_t ]. \label{eq:astrom-2}
   \end{align}$$

Using \\eqref{eq:astrom-1} and \\eqref{eq:astrom-2}, we get that the total cost can be
written as
$$ \begin{align} 
      & \EXP\bigg[ \sum_{t=1}^{T-1} \overbrace{\big[
        x_t^\TRANS Q_t x_t + u_t^\TRANS R_t u_t + x_{t+1}^\TRANS S_{t+1}
        x_{t+1} - x_t^\TRANS S_t x_t \big]}^{\text{Term 1}}
      \nonumber \\
      & \quad + 
      \EXP\bigg[ x_1^\TRANS S_1 x_t + \sum_{t=1}^{T-1} w_t S_{t+1} w_t \bigg].
      \label{eq:astrom-3}
\end{align} $$

Now, from the completion of squares lemma, we get that "Term 1" is equal to
$$ (u_t + K_t x_t)^\TRANS [R_t + B_t^\TRANS S_{t+1} B_t] (u_t + K_t x_t).$$
Substituting this back in \\eqref{eq:astrom-3}, we get \\eqref{eq:astrom}.
&nbsp;$\Box$

---

Now we can prove @thm:stochastic-lqr using @prop:astrom. Note that the second
term in \\eqref{eq:astrom} does not depend on the choice of control actions $u_t$.
Thus, in order to minimimze the total expected cost, it suffices to minimize
the first term of \\eqref{eq:astrom}. Since $R_t$ is positive definite and
$S_{t+1}$ is positive semi-definite, $R_t + B_t^\TRANS S_{t+1} B_t$ is
positive definite. Thus, the first term of \\eqref{eq:astrom} is sum of squares.
Choosing $ u_t = -K_t x_t$
sets this term to its minimum value of $0$. Hence, $u_t = -K_t x_t$ is the
optimal control strategy.

## References

The term _certainty equivalence_ is due to @Simon1956, who was looking at a
static problem; a similar result had earlier been shown by @Theil1954. A
result which is essentially equivalent to the stochastic LQR problem is proved
by @Theil1957. The model for deterministic LQR is due to @Kalman1960b, who
proved the result for continuous time systems. 

The alternative proof that does not use dynamic programming is due to
@Astrom1970.


<!--
Lemma

:   Suppose $x$ and $u$ are vectors. Consider a quadratic form
    $$ Q(x,u) = \MATRIX{ x \\ u }^\TRANS
       \MATRIX{ Q_{xx} & Q_{xu} \\ Q_{ux} & Q_{uu} }
       \MATRIX{ x \\ u }.
       $$ {#eq:quadratic}

    Assume it is symmetric and $Q_{uu}$ is [positive definite]. Then the
    minimum with respect to $u$ is achieved at 
    $$\begin{equation}\label{eq:opt-schur}
      u = - Q_{uu}^{-1} Q_{ux} x,
    \end{equation}$$
    and is equal to
    $$\begin{equation}\label{eq:schur}
      x^\TRANS [ Q_{xx} - Q_{xu} Q_{uu}^{-1} Q_{ux} ] x. 
    \end{equation}$$

The term in the square brackets in \\eqref{eq:schur} is the [Schur complement] of 
$\MATRIX{ Q_{xx} & Q_{xu} \\ Q_{ux} & Q_{uu} }$

[positive definite]: ../../appendix/positive-definite-matrix
[Schur complement]: https://en.wikipedia.org/wiki/Schur_complement

### Proof

Observe that
$$ Q(x,u) = x^\TRANS Q_{xx} x + 2 x^\TRANS Q_{xu} u + u^\TRANS Q_{uu} u$$

Taking the derivative w.r.t. $u$, we get
$$ \frac{\partial Q(x,u)}{\partial u} = 2 x^\TRANS Q_{xu} + 2 u^\TRANS
Q_{uu}
\quad \text{and} \quad
\frac{\partial^2 Q(x,u)}{\partial u^2} = Q_{uu} > 0.
$$

Thus, the optimal $u$ is given by \\eqref{eq:opt-schur} and the minimum value is
\\eqref{eq:schur}. 

## Astrom's Proof

-->
